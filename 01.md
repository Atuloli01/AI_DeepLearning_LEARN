eep learning : is a technique which basically mimics the human brain. So in 1950-60 reserchers and scientist thought can we make a machine work like how we human actually learn and we know that we will be learning from the environment with the help of our brain which has capacity to learn things very quickly. so that is where the deep learning concepts came and this thought led to the mention of something called as neural networks.

first simplest type of neural network is called perceptron. there were some problems in the perceptron and it was not able to learn very properly because of the concepts that were applied. later in 1980s researcher, scientist, teacher called Jeffrey Newton invented the concept called back propagation. because of this back propagation ANN, CNN and RNN became so efficient that many companies are now using it many people are using it and many people have developed a lot of application which are efficiently working because of Jeffrey Newton and his concepts of back propagation.
<Backpropagation>
Backpropagation is a fundamental algorithm used in the training of neural networks, especially in deep learning models. It works by calculating the gradient of the loss function (the difference between predicted and actual values) with respect to the weights of the network and adjusting those weights to minimize the loss.

How Backpropagation Works:
Forward Pass:

Input data passes through the neural network layer by layer.
The output is computed based on the current weights and biases.
Loss Calculation:

The difference between the predicted output and the actual (true) output is computed using a loss function (e.g., Mean Squared Error for regression, Cross-Entropy for classification).
Backward Pass (Backpropagation):

The error is propagated back through the network, layer by layer, in reverse order.
Gradients of the loss function with respect to each weight are calculated using the chain rule of calculus.
Each weight is updated by a small step in the direction that reduces the loss, usually using a technique like gradient descent.
Weight Update:

The weights are adjusted to reduce the error.
This process continues iteratively over many cycles (epochs), and over time, the model improves its accuracy by minimizing the loss.
Key Concepts:
Activation Function: This function (like ReLU, Sigmoid, or Tanh) introduces non-linearity into the network and is essential for learning complex patterns.
Gradient Descent: A common optimization algorithm used in backpropagation to minimize the loss function. Variations like stochastic gradient descent (SGD), mini-batch gradient descent, and Adam are used to improve performance.
Learning Rate: A hyperparameter that controls how much to adjust the weights with each iteration.
</Backpropagation>
